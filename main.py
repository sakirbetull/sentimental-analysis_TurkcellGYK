
import pandas as pd
import re
import string
import numpy as np


df = pd.read_csv(r'C:\Users\sakir\OneDrive\MasaÃ¼stÃ¼\career\turkcell\GYK-NLP\duyguanalizi.py\go_emotions_dataset.csv')
print(df.head())

'''
id: Her metin Ã¶rneÄŸi iÃ§in benzersiz kimlik
text: Analiz edilecek metin (Reddit yorumlarÄ±)
example_very_unclear: Metnin belirsiz/anlaÅŸÄ±lmaz olup olmadÄ±ÄŸÄ±nÄ± gÃ¶steren boolean deÄŸer
Duygu Kategorileri (27 adet)[pozitif-negatif- nÃ¶tr]
'''

def goemotions_clean(text):
    # 1. Ã–zel durumlarÄ± koruyarak temizle
    text = text.strip()
    
    # 2. Reddit format temizleme (dikkatli)
    text = re.sub(r'^>', '', text)  # AlÄ±ntÄ± iÅŸareti
    text = text.replace('/s', ' [SARCASM]')
    text = text.replace('/jk', ' [JOKING]')
    
    # 3. HTML entities
    text = re.sub(r'&\w+;', '', text)
    
    # 4. Fazla boÅŸluklar
    text = re.sub(r'\s+', ' ', text)
    
    # 5. [NAME], [RELIGION] gibi etiketleri KORUYUN
    
    return text.strip()

df['clean_text'] = df['text'].apply(goemotions_clean)  # MesajlarÄ± temizleme
print(df.shape)

'''
âš ï¸ YAPMAMANIZ GEREKENLER
Stop words kaldÄ±rmayÄ±n - "not", "very" gibi kelimeler duygu iÃ§in kritik
Aggressive stemming/lemmatization yapmayÄ±n - kelime anlamlarÄ± deÄŸiÅŸebilir
Emoji'leri tamamen kaldÄ±rmayÄ±n - varsa metin karÅŸÄ±lÄ±ÄŸÄ±na Ã§evirin
BÃ¼yÃ¼k/kÃ¼Ã§Ã¼k harf normalleÅŸtirmesini aÅŸÄ±rÄ± yapmayÄ±n
Contextual bilgileri ([NAME] vs gerÃ§ek isim) kaldÄ±rmayÄ±n
'''


# Duygu kolonlarÄ±nÄ± tanÄ±mla: hedef deÄŸiÅŸken haline getirme  
emotion_columns = ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 
                   'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 
                   'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 
                   'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 
                   'relief', 'remorse', 'sadness', 'surprise', 'neutral']

# Belirsiz Ã¶rnekleri filtrele 
print(f"Belirsiz Ã¶rnekler: {df['example_very_unclear'].sum()}")
df_clear = df[df['example_very_unclear'] == False].copy()
print(f"Temiz veri boyutu: {df_clear.shape}")

# Duygu daÄŸÄ±lÄ±mÄ±nÄ± kontrol et
print("\nDuygu daÄŸÄ±lÄ±mÄ±:")
emotion_counts = df_clear[emotion_columns].sum().sort_values(ascending=False)
print(emotion_counts.head(10))

# Ã‡oklu etiket kontrolÃ¼
df_clear['num_emotions'] = df_clear[emotion_columns].sum(axis=1)
print(f"\nOrtalama duygu sayÄ±sÄ± per Ã¶rnek: {df_clear['num_emotions'].mean():.2f}")
print(f"HiÃ§ duygusu olmayan Ã¶rnekler: {(df_clear['num_emotions'] == 0).sum()}")


'''
(211225, 32)
Belirsiz Ã¶rnekler: 3411
Temiz veri boyutu: (207814, 32)

Duygu daÄŸÄ±lÄ±mÄ±:
neutral        55298  --dominant
approval       17620
admiration     17131
annoyance      13618
gratitude      11625
disapproval    11424
curiosity       9692
amusement       9245
realization     8785
optimism        8715
dtype: int64

Ortalama duygu sayÄ±sÄ± per Ã¶rnek: 1.20
HiÃ§ duygusu olmayan Ã¶rnekler: 0
'''

# makine Ã¶ÄŸrenmesi -> train ve test split
from sklearn.model_selection import train_test_split

print(f"\nğŸ” Veri kontrolÃ¼:")
print(f"emotion_columns uzunluÄŸu: {len(emotion_columns)}")
print(f"df_clear[emotion_columns] shape: {df_clear[emotion_columns].shape}")

# DOÄRU: df_clear kullan ve emotion_columns hedef olarak al
X_train, X_test, y_train, y_test = train_test_split(
    df_clear['clean_text'],           # TemizlenmiÅŸ metinler
    df_clear[emotion_columns],        # SADECE 27 duygu kolonu (num_emotions DAHÄ°L DEÄÄ°L!)
    test_size=0.2, 
    random_state=42
)

print(f"\nâœ… Train-Test Split SonuÃ§larÄ±:")
print(f"X_train shape: {X_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_train shape: {y_train.shape}")  # (166251, 28) olmalÄ±
print(f"y_test shape: {y_test.shape}")    # (41563, 28) olmalÄ±

# ============================================================================
# TOKENIZATION VE PADDING - ADIM ADIM ANALÄ°Z
# ============================================================================

# tokenizer
'''
# Kelime SÃ¶zlÃ¼ÄŸÃ¼ (word_index)
{
    'i': 15,
    'love': 842, 
    'this': 23,
    'movie': 156,
    '<OOV>': 1  # Bilinmeyen kelimeler iÃ§in
}
'''
'''Tokenization (Kelime DÃ¶nÃ¼ÅŸtÃ¼rme)
Tensorflow'un Tokenizer'Ä± kullanÄ±larak metinler sayÄ±sal dizilere Ã§evriliyor
Her kelimeye benzersiz bir ID atanÄ±yor'''
from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer(num_words=15000, oov_token='<OOV>')
tokenizer.fit_on_texts(X_train)
vocab_size = len(tokenizer.word_index) + 1 # 1 ekleniyor Ã§Ã¼nkÃ¼ 0 index kullanÄ±lmaz. (Padding iÃ§in)

#metinleri sayÄ±ya Ã§evirme (sequence)
X_train_sequences = tokenizer.texts_to_sequences(X_train)
X_test_sequences = tokenizer.texts_to_sequences(X_test)

for i in range(3):
    print(f" {i+1} -Orjinal Mesaj: {X_train.iloc[i]}")

for i in range(3):
    print(f" {i+1} -Temiz Mesaj: {X_train_sequences[i]}")

# Uzunluk analizini ekle
lengths = [len(seq) for seq in X_train_sequences]

# Padding

''' Padding (Uzunluk EÅŸitleme)
Veri setine gÃ¶re optimize edilmiÅŸ uzunluk kullanÄ±lacak'''
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Dinamik max_length (veri setine gÃ¶re)
max_length = int(np.percentile(lengths, 95))  # %95'lik dilim kullan (Ã§ok uzun metinler iÃ§in)
# print(f"SeÃ§ilen max_length: {max_length}")
# print(f"Ortalama uzunluk: {np.mean(lengths):.1f}")
print(f"Medyan uzunluk: {np.median(lengths):.1f}")
print(f"Maksimum uzunluk: {max(lengths)}")

print(f"\nğŸ“Š Padding Ã¶ncesi:")
print(f"Train sequences: {len(X_train_sequences)} samples")
print(f"Test sequences: {len(X_test_sequences)} samples") 

X_train_padded = pad_sequences(X_train_sequences, maxlen=max_length, padding='post', truncating='post')
X_test_padded = pad_sequences(X_test_sequences, maxlen=max_length, padding='post', truncating='post')

print(f"\nğŸ“Š Padding sonrasÄ± - Final shapes:")
print(f"X_train_padded: {X_train_padded.shape}")
print(f"X_test_padded: {X_test_padded.shape}")
print(f"y_train: {y_train.shape}")         # (166251, 28) olmalÄ±
print(f"y_test: {y_test.shape}")           # (41563, 28) olmalÄ±

# Shape kontrolÃ¼
if y_train.shape[1] != len(emotion_columns):
    print(f"âš ï¸ UYARI: y_train ikinci boyutu {y_train.shape[1]}, {len(emotion_columns)} olmalÄ±!")
    print(f"emotion_columns: {len(emotion_columns)} element")
else:
    print(f"âœ… Shape kontrolÃ¼: y_train {y_train.shape[1]} == {len(emotion_columns)} emotions")

# VektÃ¶rleÅŸtirilmiÅŸ Ã¶rnekleri gÃ¶ster
print("\nğŸ“ Padding Ã¶rnekleri:")
for i in range(3):
    print(f" {i+1} -VektÃ¶rleÅŸtirilmiÅŸ Mesaj (ilk 15 token): {X_train_padded[i][:15]}")



'''
Metin 1: [15, 842, 23]           # 3 kelime
Metin 2: [42, 7, 199, 88, 156]   # 5 kelime
'''

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout
from tensorflow.keras.callbacks import EarlyStopping


# ============================================================================

# Modeli tanÄ±mlama
print(f"\nğŸ—ï¸ MODEL OLUÅTURMA")
print(f"Vocab size: {vocab_size}")
print(f"Max length: {max_length}")
print(f"Target shape: {len(emotion_columns)} (emotions)")

model = Sequential()

# Embedding Layer-Kelime ID'lerini anlamlÄ± vektÃ¶rlere Ã§eviriyor
model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=max_length, mask_zero=True))
print(f"âœ… Embedding layer: {vocab_size} kelime -> 128 boyutlu vektÃ¶rler")

# Bidirectional LSTM - Her iki yÃ¶nde metin analizi-sÄ±ralÄ± metin iÅŸleme
'''
# "Ali sabah okula gitti. AkÅŸam eve geldi."
# LSTM "Ali" kelimesini hatÄ±rlayarak "eve geldi" kÄ±smÄ±nda 
# kimin eve geldiÄŸini anlayabilir
'''
model.add(Bidirectional(LSTM(64)))
# (f"âœ… Bidirectional LSTM: 64x2=128 output")

model.add(Dropout(0.5))  # Dropout ekle-Overfitting'i Ã¶nlemek iÃ§in
# print(f"âœ… Dropout: 50% nÃ¶ron kapatma")

# hidden layer
model.add(Dense(128, activation='relu'))
print(f"âœ… Dense hidden: 128 nÃ¶ron (ReLU)")

# Dense Layer - 28 duygu iÃ§in sigmoid (dinamik)
model.add(Dense(len(emotion_columns), activation='sigmoid'))
print(f"âœ… Output layer: {len(emotion_columns)} duygu (sigmoid)")

print(f"\nğŸ”§ Model derleniyor...")
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
# print(f"âœ… Model hazÄ±r! (Adam + binary_crossentropy)")
# print(f"\nğŸ“‹ MODEL Ã–ZETÄ°:")
model.summary()

print(f"\nâ° EÄÄ°TÄ°M BAÅLIYOR...")
print(f"Early Stopping: val_loss 5 epoch iyileÅŸmezse dur")

early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Final shape kontrolÃ¼ - 28 duygu iÃ§in dÃ¼zeltildi
assert y_train.shape[1] == len(emotion_columns), f"y_train shape hatalÄ±: {y_train.shape}, (?, {len(emotion_columns)}) olmalÄ±"
assert y_test.shape[1] == len(emotion_columns), f"y_test shape hatalÄ±: {y_test.shape}, (?, {len(emotion_columns)}) olmalÄ±"
print(f"âœ… Shape kontrolÃ¼ baÅŸarÄ±lÄ±! {len(emotion_columns)} emotions")

history = model.fit(
    X_train_padded, y_train, 
    epochs=30, 
    batch_size=32,
    validation_data=(X_test_padded, y_test), 
    callbacks=[early_stopping],
    verbose=1
)

print(f"\nğŸ‰ EÄÄ°TÄ°M TAMAMLANDI!")

# EÄŸitim sonunda modeli kaydet.
model.save("duyguanalizi_model.h5")
print("âœ… Model kaydedildi: duyguanalizi_model.h5")

# Emotion mapping'i kaydet (tahmin sÄ±rasÄ±nda kullanmak iÃ§in)
emotion_mapping = {i: emotion for i, emotion in enumerate(emotion_columns)}

import pickle

# Tokenizer'Ä± kaydet - Gelecekte aynÄ± preprocessing iÃ§in
'''
tokenizer objesini (eÄŸitim sÄ±rasÄ±nda oluÅŸturulan kelime sÃ¶zlÃ¼ÄŸÃ¼yle birlikte) dosyaya kaydeder
'wb' = write binary (ikili yazma modu)
.pkl = pickle formatÄ±nda dosya uzantÄ±sÄ±

Neden gerekli?
1. Yeni metinlerde aynÄ± kelime-ID eÅŸleÅŸmesini kullanmak iÃ§in
2. Model prodÃ¼ksiyon ortamÄ±nda tutarlÄ± tahminler yapabilmek iÃ§in  
3. EÄŸitim sÄ±rasÄ±ndaki preprocessing'i birebir tekrarlamak iÃ§in
'''

with open('emotion_tokenizer.pkl', 'wb') as f:
    pickle.dump(tokenizer, f)
print("âœ… Tokenizer kaydedildi: emotion_tokenizer.pkl")

# Emotion mapping'i de kaydet
with open('emotion_mapping.pkl', 'wb') as f:
    pickle.dump(emotion_mapping, f)
print("âœ… Emotion mapping kaydedildi: emotion_mapping.pkl")

# Model config'i kaydet
config = {
    'max_length': max_length,
    'vocab_size': vocab_size,
    'emotion_columns': emotion_columns
}
with open('model_config.pkl', 'wb') as f:
    pickle.dump(config, f)
print("âœ… Model config kaydedildi: model_config.pkl")

import matplotlib.pyplot as plt

# Accuracy
plt.figure(figsize=(12, 4))

# Accuracy grafiÄŸi
plt.subplot(1, 3, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy', linewidth=2, color='blue')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2, color='red')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# cÃ¼mle iÃ§inde iki duygu varsa tahmini nasÄ±l yapÄ±yor